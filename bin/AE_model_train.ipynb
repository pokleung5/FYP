{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainHelper\n",
    "import utils\n",
    "from mds.lmds import landmarkMDS\n",
    "from mds.cmds import classicalMDS\n",
    "from mds.fastmap import fastmap\n",
    "import numpy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import lossFunction as lossF\n",
    "from model.AutoEncoder import AutoEncoder\n",
    "from model.DynParam import DynParam\n",
    "from model.Linear import Linear\n",
    "from model.VAE import VAE\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils']);\n",
    "reload(sys.modules['trainHelper']);\n",
    "reload(sys.modules['lossFunction']);\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = (1000000, 100)\n",
    "ss, N, d = 1600, 10, 2\n",
    "\n",
    "try:\n",
    "    euclidean_data1 = utils.load_variable('data/euclidean_data1.pkl')\n",
    "    euclidean_data2 = utils.load_variable('data/euclidean_data2.pkl')\n",
    "    \n",
    "    rand_data1 = utils.load_variable('data/rand_data.pkl')\n",
    "    rand_data2 = utils.load_variable('data/rand_data.pkl')\n",
    "    \n",
    "    if euclidean_data.size() != (ss, 1, N, N):\n",
    "        print(\"Updated data for requirement !\")\n",
    "        raise Exception(\"Previous data not match requirement !\")\n",
    "\n",
    "except:\n",
    "    euclidean_data1 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    euclidean_data2 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data1 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data2 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    utils.dump_variable(euclidean_data1, 'data/euclidean_data1.pkl')\n",
    "    utils.dump_variable(euclidean_data2, 'data/euclidean_data2.pkl')\n",
    "\n",
    "    utils.dump_variable(rand_data1, 'data/rand_data1.pkl')\n",
    "    utils.dump_variable(rand_data2, 'data/rand_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cmds_loss: \t tensor(3384.4374) | 1000 success\nfastmap_loss: \t tensor(8198.4327) | 1000 success\n"
    }
   ],
   "source": [
    "test_batch = 1000\n",
    "test_data = utils.generate_rand_DM(\n",
    "                N=N,\n",
    "                sample_size=test_batch, \n",
    "                sample_space=sample_space, isInt=True, \n",
    "                v_size=2, \n",
    "                transform_func=lambda x: x[0] + x[1])\n",
    "\n",
    "test_data = test_data.view(test_batch, 1, N, N)\n",
    "test_data = utils.minmax_norm(test_data, dmin=0)[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "cmds_loss, fastmap_loss = [], []\n",
    "\n",
    "for d in test_data:\n",
    "\n",
    "    d1 = numpy.array(d[0].data)\n",
    "\n",
    "    cmds_rs = classicalMDS(d1, 2)\n",
    "    cmds_rs = torch.tensor(cmds_rs)\n",
    "\n",
    "    cmds_dm = utils.get_distance_matrix(cmds_rs)\n",
    "    cmds_dm = utils.minmax_norm(cmds_dm, dmin=0)[0]\n",
    "\n",
    "    cmds_loss.append(torch.sum((cmds_dm - d)** 2))\n",
    "\n",
    "    fastmap_rs = fastmap(d1, 2)\n",
    "    fastmap_rs = torch.tensor(fastmap_rs)\n",
    "    \n",
    "    fastmap_dm = utils.get_distance_matrix(fastmap_rs)\n",
    "    fastmap_dm = utils.minmax_norm(fastmap_dm, dmin=0)[0]\n",
    "\n",
    "    fastmap_loss.append(torch.sum((fastmap_dm - d)** 2))\n",
    "\n",
    "\n",
    "print(\"cmds_loss: \\t\", \n",
    "        torch.tensor(cmds_loss).sum(), \"|\" , \n",
    "        len(cmds_loss), \"success\")\n",
    "\n",
    "print(\"fastmap_loss: \\t\", \n",
    "        torch.tensor(fastmap_loss).sum(), \"|\" , \n",
    "        len(fastmap_loss), \"success\")\n",
    "\n",
    "\n",
    "# cmds_loss: \t tensor(12066.6691) | 1000 success\n",
    "# fastmap_loss: \t tensor(73897.4167) | 1000 success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "backup\\AE_SGD_3_64_distance_900.model    \t\t\t| tensor(2310.0738)\nbackup\\AE_SGD_2_64_distance_900.model    \t\t\t| tensor(2374.3670)\nbackup\\AE_SGD_3_64_distance_600.model    \t\t\t| tensor(2427.4779)\nbackup\\AE_SGD_2_64_distance_600.model    \t\t\t| tensor(2486.6837)\nbackup\\AE_SGD_2_32_distance_900.model    \t\t\t| tensor(2656.1486)\nbackup\\AE_SGD_2_64_distance_300.model    \t\t\t| tensor(2729.6968)\nbackup\\AE_SGD_2_32_distance_600.model    \t\t\t| tensor(2810.5057)\nbackup\\AE_SGD_2_32_distance_300.model    \t\t\t| tensor(3226.2111)\nbackup\\AE_SGD_3_64_distance_300.model    \t\t\t| tensor(3249.5551)\nbackup\\AE_SGD_3_32_distance_900.model    \t\t\t| tensor(3954.2415)\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_model(model, preprocess, test_data, target_dm):\n",
    "\n",
    "    t_data = preprocess(test_data)\n",
    "    t_data = utils.minmax_norm(t_data, dmin=0)[0]\n",
    "\n",
    "    model_rs = model(t_data)\n",
    "    # model_rs = model_rs.view(model_rs.size()[0], *target_dm)\n",
    "    model_dm = torch.stack([\n",
    "        utils.unvectorize_distance(m, N) for m in model_rs\n",
    "    ])\n",
    "    \n",
    "    score = []\n",
    "\n",
    "    for dm in model_dm:\n",
    "\n",
    "        rs = classicalMDS(dm[0].data, 2)\n",
    "        rs = torch.tensor(rs)\n",
    "\n",
    "        rs_dm = utils.get_distance_matrix(rs)\n",
    "        rs_dm = utils.minmax_norm(rs_dm, dmin=0)[0]\n",
    "\n",
    "        loss = (rs_dm - dm.view_as(rs_dm)) ** 2\n",
    "\n",
    "        score.append(torch.sum(loss))\n",
    "\n",
    "    return torch.tensor(score).sum()\n",
    "\n",
    "    # loss = loss.view(loss.size()[0], 1, -1)\n",
    "    # losssumMSE = torch.sum(loss, dim=2) ** 2\n",
    "    # return torch.sum(losssumMSE)\n",
    "\n",
    "\n",
    "result_score = []\n",
    "\n",
    "for filepath in glob.iglob('backup/AE*.model'):\n",
    "    h: trainHelper.TrainHelper = utils.load_variable(filepath)\n",
    "    result_score.append([\n",
    "        filepath, \n",
    "        test_model(h.model, h.preprocess, test_data, (N, 2))\n",
    "        ])\n",
    "\n",
    "result_score = sorted(result_score, key=lambda x: x[1])\n",
    "\n",
    "for rss in result_score[:10]:\n",
    "    print(rss[0], \"   \\t\\t\\t|\", rss[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([\n",
    "            euclidean_data1,\n",
    "            euclidean_data2,\n",
    "            # rand_data1,\n",
    "            # rand_data2,\n",
    "        ])\n",
    "        \n",
    "data = data.view(ss * 2, 1, N, N)\n",
    "\n",
    "data = utils.minmax_norm(data, dmin=0)[0]\n",
    "\n",
    "batch = 32\n",
    "dlr = DataLoader(data, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " Mean loss: 315.31459514641705\n510 \t| Mean loss: 315.3145841745517\n520 \t| Mean loss: 315.3145847307575\n530 \t| Mean loss: 315.31458143870543\n540 \t| Mean loss: 315.314577852435\n550 \t| Mean loss: 315.3145640460267\n560 \t| Mean loss: 315.3145686338578\n570 \t| Mean loss: 315.31456886220866\n580 \t| Mean loss: 315.3145646147291\n590 \t| Mean loss: 315.31456089831426\nTime used for the training:  212.30386300000004 s\nTest Result:  tensor(13558.5307, grad_fn=<SumBackward0>)\nTraining  AE_SGD_4_32_distance\n600 \t| Mean loss: 315.3145578244642\n610 \t| Mean loss: 315.3145590783664\n620 \t| Mean loss: 315.3145566692312\n630 \t| Mean loss: 315.3145489875231\n640 \t| Mean loss: 315.3145505006942\n650 \t| Mean loss: 315.31454035557704\n660 \t| Mean loss: 315.314532603628\n670 \t| Mean loss: 315.3145296792866\n680 \t| Mean loss: 315.3145298528115\n690 \t| Mean loss: 315.31452619208284\n700 \t| Mean loss: 315.31452256535744\n710 \t| Mean loss: 315.3145171905174\n720 \t| Mean loss: 315.3145183338085\n730 \t| Mean loss: 315.3145165027522\n740 \t| Mean loss: 315.3145119367337\n750 \t| Mean loss: 315.3144991281789\n760 \t| Mean loss: 315.31451185587946\n770 \t| Mean loss: 315.31450494558334\n780 \t| Mean loss: 315.3145000834899\n790 \t| Mean loss: 315.3144987235287\n800 \t| Mean loss: 315.31449207704486\n810 \t| Mean loss: 315.31449477393454\n820 \t| Mean loss: 315.31448760131286\n830 \t| Mean loss: 315.3144838302412\n840 \t| Mean loss: 315.3144782809343\n850 \t| Mean loss: 315.3144809137025\n860 \t| Mean loss: 315.3144691022492\n870 \t| Mean loss: 315.3144756140931\n880 \t| Mean loss: 315.31446943027805\n890 \t| Mean loss: 315.31446613542727\nTime used for the training:  320.0081610000001 s\nTest Result:  tensor(13558.6617, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_32_distance\n0 \t| Mean loss: 394.66620577807697\n10 \t| Mean loss: 316.5875361413505\n20 \t| Mean loss: 316.45802450417665\n30 \t| Mean loss: 315.50686023804576\n40 \t| Mean loss: 315.4993394035352\n50 \t| Mean loss: 315.372446320049\n60 \t| Mean loss: 315.3564527692422\n70 \t| Mean loss: 315.3547636092242\n80 \t| Mean loss: 315.3547472891508\n90 \t| Mean loss: 315.3547545219672\n100 \t| Mean loss: 315.3547413947683\n110 \t| Mean loss: 315.3547240534324\n120 \t| Mean loss: 315.35473062452917\n130 \t| Mean loss: 315.3547191095364\n140 \t| Mean loss: 315.3547169400206\n150 \t| Mean loss: 315.3546978019743\n160 \t| Mean loss: 315.3547018196022\n170 \t| Mean loss: 315.354694339593\n180 \t| Mean loss: 315.3546922796351\n190 \t| Mean loss: 315.3546767312973\n200 \t| Mean loss: 315.354670241359\n210 \t| Mean loss: 315.35465465211115\n220 \t| Mean loss: 315.35465659020247\n230 \t| Mean loss: 315.3546545398487\n240 \t| Mean loss: 315.354644558214\n250 \t| Mean loss: 315.3546376152992\n260 \t| Mean loss: 315.35462661048257\n270 \t| Mean loss: 315.35462691279173\n280 \t| Mean loss: 315.35462058914715\n290 \t| Mean loss: 315.3546098253423\nTime used for the training:  124.819213 s\nTest Result:  tensor(13252.0061, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_32_distance\n300 \t| Mean loss: 315.35461744319696\n310 \t| Mean loss: 315.35460520777787\n320 \t| Mean loss: 315.3545922699342\n330 \t| Mean loss: 315.3546006402389\n340 \t| Mean loss: 315.3545858334805\n350 \t| Mean loss: 315.3545873528943\n360 \t| Mean loss: 315.35457205271103\n370 \t| Mean loss: 315.3545721472673\n380 \t| Mean loss: 315.35456109369056\n390 \t| Mean loss: 315.35455418224234\n400 \t| Mean loss: 315.3545540972814\n410 \t| Mean loss: 315.354552127346\n420 \t| Mean loss: 315.35454702745295\n430 \t| Mean loss: 315.35454213261767\n440 \t| Mean loss: 315.3545430442041\n450 \t| Mean loss: 315.35452419131633\n460 \t| Mean loss: 315.3545316642331\n470 \t| Mean loss: 315.35452268277385\n480 \t| Mean loss: 315.3545195315421\n490 \t| Mean loss: 315.3545217075647\n500 \t| Mean loss: 315.35451498078356\n510 \t| Mean loss: 315.35450189213117\n520 \t| Mean loss: 315.35450588100804\n530 \t| Mean loss: 315.3545011812615\n540 \t| Mean loss: 315.3544912743286\n550 \t| Mean loss: 315.35449270531905\n560 \t| Mean loss: 315.3544986147046\n570 \t| Mean loss: 315.35448584164465\n580 \t| Mean loss: 315.3544861961449\n590 \t| Mean loss: 315.35447732497835\nTime used for the training:  249.87472000000002 s\nTest Result:  tensor(13252.0413, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_32_distance\n600 \t| Mean loss: 315.3544761768215\n610 \t| Mean loss: 315.35446770693903\n620 \t| Mean loss: 315.3544697980376\n630 \t| Mean loss: 315.35447502376405\n640 \t| Mean loss: 315.35446329162386\n650 \t| Mean loss: 315.3544523505301\n660 \t| Mean loss: 315.35446291564193\n670 \t| Mean loss: 315.3544568435074\n680 \t| Mean loss: 315.35445435151615\n690 \t| Mean loss: 315.3544537064961\n700 \t| Mean loss: 315.3544402315884\n710 \t| Mean loss: 315.35444948486423\n720 \t| Mean loss: 315.3544391116116\n730 \t| Mean loss: 315.3544407603686\n740 \t| Mean loss: 315.3544412569291\n750 \t| Mean loss: 315.35443759934896\n760 \t| Mean loss: 315.3544182772951\n770 \t| Mean loss: 315.35442165711663\n780 \t| Mean loss: 315.3544304660206\n790 \t| Mean loss: 315.354430287401\n800 \t| Mean loss: 315.3544145341417\n810 \t| Mean loss: 315.3544218006462\n820 \t| Mean loss: 315.35441083527064\n830 \t| Mean loss: 315.35440369032955\n840 \t| Mean loss: 315.35440609708627\n850 \t| Mean loss: 315.35440327715236\n860 \t| Mean loss: 315.3544014166229\n870 \t| Mean loss: 315.35439505688805\n880 \t| Mean loss: 315.3544010174665\n890 \t| Mean loss: 315.3543997628905\nTime used for the training:  374.791538 s\nTest Result:  tensor(13252.0708, grad_fn=<SumBackward0>)\nTraining  AE_SGD_2_64_distance\n0 \t| Mean loss: 389.7646829821199\n10 \t| Mean loss: 263.8760401051606\n20 \t| Mean loss: 169.32436587673357\n30 \t| Mean loss: 136.843688011745\n40 \t| Mean loss: 118.3647558370074\n50 \t| Mean loss: 99.36669946240708\n60 \t| Mean loss: 89.03079018358035\n70 \t| Mean loss: 83.58669759254035\n80 \t| Mean loss: 77.67846119222378\n90 \t| Mean loss: 74.80256953514659\n100 \t| Mean loss: 68.33168123826283\n110 \t| Mean loss: 65.54560022535988\n120 \t| Mean loss: 64.50355944944413\n130 \t| Mean loss: 62.76755253032586\n140 \t| Mean loss: 58.52759408249611\n150 \t| Mean loss: 55.253974615236785\n160 \t| Mean loss: 53.53857365567579\n170 \t| Mean loss: 52.55388641016244\n180 \t| Mean loss: 51.7022589266506\n190 \t| Mean loss: 50.8096649241975\n200 \t| Mean loss: 49.86889701517804\n210 \t| Mean loss: 49.06543644999472\n220 \t| Mean loss: 48.17368276364066\n230 \t| Mean loss: 47.32836100429099\n240 \t| Mean loss: 46.51416816401181\n250 \t| Mean loss: 45.73420868827367\n260 \t| Mean loss: 44.91464357726575\n270 \t| Mean loss: 44.13300870968149\n280 \t| Mean loss: 43.53027954230164\n290 \t| Mean loss: 42.81397409929578\nTime used for the training:  81.00810200000001 s\nTest Result:  tensor(11669.8093, grad_fn=<SumBackward0>)\nTraining  AE_SGD_2_64_distance\n300 \t| Mean loss: 42.22260424599612\n310 \t| Mean loss: 41.59867704243458\n320 \t| Mean loss: 41.068423376903\n330 \t| Mean loss: 40.4018294367185\n340 \t| Mean loss: 39.813935274223574\n350 \t| Mean loss: 39.19801854899112\n360 \t| Mean loss: 38.73453855557808\n370 \t| Mean loss: 38.101598096279986\n380 \t| Mean loss: 37.61041994418081\n390 \t| Mean loss: 37.17153098460372\n400 \t| Mean loss: 36.624777579301046\n410 \t| Mean loss: 36.18899886235825\n420 \t| Mean loss: 35.75764331579328\n430 \t| Mean loss: 35.284640186642356\n440 \t| Mean loss: 34.977138466106744\n450 \t| Mean loss: 34.580045519245026\n460 \t| Mean loss: 34.28258890437823\n470 \t| Mean loss: 34.00048791847672\n480 \t| Mean loss: 33.750803256178386\n490 \t| Mean loss: 33.55329025301046\n500 \t| Mean loss: 33.22757026922382\n510 \t| Mean loss: 32.9518286517621\n520 \t| Mean loss: 32.68871013421355\n530 \t| Mean loss: 32.46283909113701\n540 \t| Mean loss: 32.25148066094885\n550 \t| Mean loss: 31.97474828580879\n560 \t| Mean loss: 31.70481234251914\n570 \t| Mean loss: 31.54843213210672\n580 \t| Mean loss: 31.3408662454592\n590 \t| Mean loss: 31.022573015189366\nTime used for the training:  164.591048 s\nTest Result:  tensor(11837.0352, grad_fn=<SumBackward0>)\nTraining  AE_SGD_2_64_distance\n600 \t| Mean loss: 30.872586929638633\n610 \t| Mean loss: 30.625310214390993\n620 \t| Mean loss: 30.428411956926315\n630 \t| Mean loss: 30.25971175987586\n640 \t| Mean loss: 30.106072177827556\n650 \t| Mean loss: 29.911128754942997\n660 \t| Mean loss: 29.712992793064785\n670 \t| Mean loss: 29.546850691501387\n680 \t| Mean loss: 29.395598001133262\n690 \t| Mean loss: 29.28209191977745\n700 \t| Mean loss: 29.078897938412016\n710 \t| Mean loss: 28.97105537386111\n720 \t| Mean loss: 28.884345594147398\n730 \t| Mean loss: 28.72939448051397\n740 \t| Mean loss: 28.65538397999881\n750 \t| Mean loss: 28.47406548679148\n760 \t| Mean loss: 28.33612846624962\n770 \t| Mean loss: 28.25269410651988\n780 \t| Mean loss: 28.071314386022262\n790 \t| Mean loss: 27.963333214415307\n800 \t| Mean loss: 27.841187328680792\n810 \t| Mean loss: 27.664236693472745\n820 \t| Mean loss: 27.673508899021858\n830 \t| Mean loss: 27.46864937264179\n840 \t| Mean loss: 27.425957049289693\n850 \t| Mean loss: 27.230329446487033\n860 \t| Mean loss: 27.124953206093714\n870 \t| Mean loss: 27.005714502497415\n880 \t| Mean loss: 26.9074362034911\n890 \t| Mean loss: 26.82569177036278\nTime used for the training:  247.94209600000005 s\nTest Result:  tensor(11894.1299, grad_fn=<SumBackward0>)\nTraining  AE_SGD_3_64_distance\n0 \t| Mean loss: 387.90018003336627\n10 \t| Mean loss: 316.21650476609534\n20 \t| Mean loss: 315.5763705567669\n30 \t| Mean loss: 294.47030791157243\n40 \t| Mean loss: 289.56505408368264\n50 \t| Mean loss: 252.82944140813183\n60 \t| Mean loss: 223.21594847386007\n70 \t| Mean loss: 200.30555072857663\n80 \t| Mean loss: 182.70462282377565\n90 \t| Mean loss: 164.60039517370285\n100 \t| Mean loss: 147.73502790698493\n110 \t| Mean loss: 144.57123985359343\n120 \t| Mean loss: 139.2257625158224\n130 \t| Mean loss: 128.01863141073636\n140 \t| Mean loss: 121.73181540784645\n150 \t| Mean loss: 118.55780967502713\n160 \t| Mean loss: 116.24319417459029\n170 \t| Mean loss: 113.68701546512544\n180 \t| Mean loss: 108.68987531872621\n190 \t| Mean loss: 106.85788633517235\n200 \t| Mean loss: 101.46978937482594\n210 \t| Mean loss: 100.57763209180106\n220 \t| Mean loss: 99.92244808121409\n230 \t| Mean loss: 99.26297164186772\n240 \t| Mean loss: 98.35546361268246\n250 \t| Mean loss: 97.12526570834245\n260 \t| Mean loss: 91.80977529822036\n270 \t| Mean loss: 90.20467996010002\n280 \t| Mean loss: 86.13207863034228\n290 \t| Mean loss: 84.0693446205727\nTime used for the training:  114.31282800000001 s\nTest Result:  tensor(11766.0714, grad_fn=<SumBackward0>)\nTraining  AE_SGD_3_64_distance\n300 \t| Mean loss: 83.33859964645852\n310 \t| Mean loss: 82.20463847644223\n320 \t| Mean loss: 79.39168693696266\n330 \t| Mean loss: 76.25775299666039\n340 \t| Mean loss: 75.15339926548155\n350 \t| Mean loss: 74.27864108994055\n360 \t| Mean loss: 73.4532483516549\n370 \t| Mean loss: 71.98322872544729\n380 \t| Mean loss: 69.96445085414054\n390 \t| Mean loss: 66.16199555954377\n400 \t| Mean loss: 64.96418386008197\n410 \t| Mean loss: 63.34096818405386\n420 \t| Mean loss: 60.861198161626255\n430 \t| Mean loss: 59.54083378885262\n440 \t| Mean loss: 58.34303399238901\n450 \t| Mean loss: 57.6646922418535\n460 \t| Mean loss: 57.08023880490045\n470 \t| Mean loss: 55.87836957239133\n480 \t| Mean loss: 55.3436624389189\n490 \t| Mean loss: 54.830590308870775\n500 \t| Mean loss: 53.81879272490061\n510 \t| Mean loss: 53.05665355882954\n520 \t| Mean loss: 52.19773241920213\n530 \t| Mean loss: 51.70122556133021\n540 \t| Mean loss: 50.89615295586266\n550 \t| Mean loss: 50.26329019007667\n560 \t| Mean loss: 49.692444813712065\n570 \t| Mean loss: 49.314292001716446\n580 \t| Mean loss: 48.682793499041516\n590 \t| Mean loss: 48.09666378470607\nTime used for the training:  228.62485300000003 s\nTest Result:  tensor(11784.1073, grad_fn=<SumBackward0>)\nTraining  AE_SGD_3_64_distance\n600 \t| Mean loss: 47.94710249730311\n610 \t| Mean loss: 47.132632474707144\n620 \t| Mean loss: 46.81641496882367\n630 \t| Mean loss: 46.49706172201864\n640 \t| Mean loss: 46.083494952916816\n650 \t| Mean loss: 45.82296373749926\n660 \t| Mean loss: 45.39333237276861\n670 \t| Mean loss: 45.135366696770106\n680 \t| Mean loss: 44.69916676524409\n690 \t| Mean loss: 44.42695350331762\n700 \t| Mean loss: 44.09790623947794\n710 \t| Mean loss: 43.74178061920761\n720 \t| Mean loss: 43.00958731468024\n730 \t| Mean loss: 42.1023212873289\n740 \t| Mean loss: 40.88616033363205\n750 \t| Mean loss: 40.14318537662112\n760 \t| Mean loss: 39.75326784651988\n770 \t| Mean loss: 39.420824408835436\n780 \t| Mean loss: 39.04436776290062\n790 \t| Mean loss: 38.753212599362044\n800 \t| Mean loss: 38.47704379207386\n810 \t| Mean loss: 38.033740769680115\n820 \t| Mean loss: 37.78978998961403\n830 \t| Mean loss: 37.453638502580446\n840 \t| Mean loss: 37.17358460693592\n850 \t| Mean loss: 36.79029465624659\n860 \t| Mean loss: 36.407435761414895\n870 \t| Mean loss: 35.51168344412644\n880 \t| Mean loss: 34.46946966186977\n890 \t| Mean loss: 33.928919758682525\nTime used for the training:  337.511901 s\nTest Result:  tensor(11882.9956, grad_fn=<SumBackward0>)\nTraining  AE_SGD_4_64_distance\n0 \t| Mean loss: 394.4895965547854\n10 \t| Mean loss: 316.43593107341513\n20 \t| Mean loss: 315.5204281968587\n30 \t| Mean loss: 315.4732247137382\n40 \t| Mean loss: 315.33677810705905\n50 \t| Mean loss: 315.32060308736664\n60 \t| Mean loss: 315.318995354722\n70 \t| Mean loss: 315.3189923345135\n80 \t| Mean loss: 315.31897450646824\n90 \t| Mean loss: 315.3189624089453\n100 \t| Mean loss: 315.31895337667015\n110 \t| Mean loss: 315.3189451303487\n120 \t| Mean loss: 315.3189266423236\n130 \t| Mean loss: 315.3189157675647\n140 \t| Mean loss: 315.31890044047555\n150 \t| Mean loss: 315.3188970846058\n160 \t| Mean loss: 315.31887823032304\n170 \t| Mean loss: 315.3188643471668\n180 \t| Mean loss: 315.3188626941483\n190 \t| Mean loss: 315.31884951443516\n200 \t| Mean loss: 315.3188420790163\n210 \t| Mean loss: 315.3188346677052\n220 \t| Mean loss: 315.3188216304166\n230 \t| Mean loss: 315.31881074386644\n240 \t| Mean loss: 315.31879586566174\n250 \t| Mean loss: 315.31878753118696\n260 \t| Mean loss: 315.31878893644637\n270 \t| Mean loss: 315.3187685124085\n280 \t| Mean loss: 315.31876697750516\n290 \t| Mean loss: 315.31874567588613\nTime used for the training:  135.11895900000002 s\nTest Result:  tensor(12514.6336, grad_fn=<SumBackward0>)\nTraining  AE_SGD_4_64_distance\n300 \t| Mean loss: 315.31874783454214\n310 \t| Mean loss: 315.3187328138497\n320 \t| Mean loss: 315.3187190709095\n330 \t| Mean loss: 315.3187125817256\n340 \t| Mean loss: 315.3187170893801\n350 \t| Mean loss: 315.31870198152205\n360 \t| Mean loss: 315.3186936838695\n370 \t| Mean loss: 315.31869409711203\n380 \t| Mean loss: 315.3186720443049\n390 \t| Mean loss: 315.3186661228528\n400 \t| Mean loss: 315.31866584035606\n410 \t| Mean loss: 315.3186659236345\n420 \t| Mean loss: 315.31864448071826\n430 \t| Mean loss: 315.3186411764361\n440 \t| Mean loss: 315.31863341872764\n450 \t| Mean loss: 315.3186351383854\n460 \t| Mean loss: 315.31861932499095\n470 \t| Mean loss: 315.318625484399\n480 \t| Mean loss: 315.3186074408368\n490 \t| Mean loss: 315.3185992525628\n500 \t| Mean loss: 315.3185875108751\n510 \t| Mean loss: 315.3185849496218\n520 \t| Mean loss: 315.3185784221294\n530 \t| Mean loss: 315.31857277226123\n540 \t| Mean loss: 315.31856188828067\n550 \t| Mean loss: 315.3185552303576\n560 \t| Mean loss: 315.31854798115074\n570 \t| Mean loss: 315.3185512514835\n580 \t| Mean loss: 315.3185400221106\n590 \t| Mean loss: 315.31853546618856\nTime used for the training:  270.921706 s\nTest Result:  tensor(12514.8847, grad_fn=<SumBackward0>)\nTraining  AE_SGD_4_64_distance\n600 \t| Mean loss: 315.3185286970755\n610 \t| Mean loss: 315.31852136329576\n620 \t| Mean loss: 315.3185171920019\n630 \t| Mean loss: 315.31851055911176\n640 \t| Mean loss: 315.31850309251536\n650 \t| Mean loss: 315.31849692176615\n660 \t| Mean loss: 315.31848881773476\n670 \t| Mean loss: 315.31849571829514\n680 \t| Mean loss: 315.3184843191277\n690 \t| Mean loss: 315.31846907188253\n700 \t| Mean loss: 315.3184711344924\n710 \t| Mean loss: 315.31847157578466\n720 \t| Mean loss: 315.3184611117207\n730 \t| Mean loss: 315.3184565546503\n740 \t| Mean loss: 315.31845010170707\n750 \t| Mean loss: 315.3184492917246\n760 \t| Mean loss: 315.318439060821\n770 \t| Mean loss: 315.31843776828407\n780 \t| Mean loss: 315.31843244606364\n790 \t| Mean loss: 315.31842583408775\n800 \t| Mean loss: 315.3184275487329\n810 \t| Mean loss: 315.31841818803196\n820 \t| Mean loss: 315.3184124860188\n830 \t| Mean loss: 315.3184126634484\n840 \t| Mean loss: 315.31839698747376\n850 \t| Mean loss: 315.31839445361084\n860 \t| Mean loss: 315.31839358231457\n870 \t| Mean loss: 315.3183857358834\n880 \t| Mean loss: 315.31838756803756\n890 \t| Mean loss: 315.3183876268171\nTime used for the training:  406.491611 s\nTest Result:  tensor(12515.2176, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_64_distance\n0 \t| Mean loss: 378.9632415121108\n10 \t| Mean loss: 316.43902572030004\n20 \t| Mean loss: 315.56221418766904\n30 \t| Mean loss: 315.51406815461144\n40 \t| Mean loss: 315.37258029530824\n50 \t| Mean loss: 315.3566142737569\n60 \t| Mean loss: 315.35496043611363\n70 \t| Mean loss: 315.35495129030875\n80 \t| Mean loss: 315.3549423961535\n90 \t| Mean loss: 315.3549366384217\n100 \t| Mean loss: 315.35492747723737\n110 \t| Mean loss: 315.35491151377306\n120 \t| Mean loss: 315.3549010978732\n130 \t| Mean loss: 315.3549000201339\n140 \t| Mean loss: 315.3548907867047\n150 \t| Mean loss: 315.3548938784691\n160 \t| Mean loss: 315.3548753607195\n170 \t| Mean loss: 315.3548699819024\n180 \t| Mean loss: 315.3548539835025\n190 \t| Mean loss: 315.3548593303316\n200 \t| Mean loss: 315.3548561795519\n210 \t| Mean loss: 315.35484042245326\n220 \t| Mean loss: 315.3548293440406\n230 \t| Mean loss: 315.3548270740837\n240 \t| Mean loss: 315.35482087403983\n250 \t| Mean loss: 315.3548147777404\n260 \t| Mean loss: 315.3548186866006\n270 \t| Mean loss: 315.35481091660284\n280 \t| Mean loss: 315.35480103387687\n290 \t| Mean loss: 315.354790622825\nTime used for the training:  228.770988 s\nTest Result:  tensor(12305.1418, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_64_distance\n300 \t| Mean loss: 315.35478206068854\n310 \t| Mean loss: 315.35477541513404\n320 \t| Mean loss: 315.3547664136861\n330 \t| Mean loss: 315.3547621784658\n340 \t| Mean loss: 315.35475879967964\n350 \t| Mean loss: 315.35475576191044\n360 \t| Mean loss: 315.35474125616315\n370 \t| Mean loss: 315.35474098240985\n380 \t| Mean loss: 315.35473994310576\n390 \t| Mean loss: 315.35473580120396\n400 \t| Mean loss: 315.3547221080161\n410 \t| Mean loss: 315.35472513171817\n420 \t| Mean loss: 315.35471787693365\n430 \t| Mean loss: 315.3547078771302\n440 \t| Mean loss: 315.35469977294736\n450 \t| Mean loss: 315.3546939413881\n460 \t| Mean loss: 315.354698193984\n470 \t| Mean loss: 315.3546876321259\n480 \t| Mean loss: 315.35468600505914\n490 \t| Mean loss: 315.35468266902893\n500 \t| Mean loss: 315.3546865898856\n510 \t| Mean loss: 315.3546796697532\n520 \t| Mean loss: 315.35466718743953\n530 \t| Mean loss: 315.3546731131577\n540 \t| Mean loss: 315.35466794894876\n550 \t| Mean loss: 315.3546574847133\n560 \t| Mean loss: 315.3546540940655\n570 \t| Mean loss: 315.35464991726514\n580 \t| Mean loss: 315.35464872346057\n590 \t| Mean loss: 315.35464404051277\nTime used for the training:  442.081639 s\nTest Result:  tensor(12305.1339, grad_fn=<SumBackward0>)\nTraining  AE_SGD_5_64_distance\n600 \t| Mean loss: 315.3546423260308\n610 \t| Mean loss: 315.35463645831913\n620 \t| Mean loss: 315.35463334720583\n630 \t| Mean loss: 315.35463189995676\n640 \t| Mean loss: 315.3546269055576\n650 \t| Mean loss: 315.35462048511204\n660 \t| Mean loss: 315.35462908116943\n670 \t| Mean loss: 315.3546150678344\n680 \t| Mean loss: 315.35461369777414\n690 \t| Mean loss: 315.3546205209491\n700 \t| Mean loss: 315.35461674578846\n710 \t| Mean loss: 315.3546050840941\n720 \t| Mean loss: 315.35460490758146\n730 \t| Mean loss: 315.3545988915783\n740 \t| Mean loss: 315.3545993939809\n750 \t| Mean loss: 315.35459325013267\n760 \t| Mean loss: 315.35459334010176\n770 \t| Mean loss: 315.354601623085\n780 \t| Mean loss: 315.354591362777\n790 \t| Mean loss: 315.35458576771435\n800 \t| Mean loss: 315.35458664882174\n810 \t| Mean loss: 315.3545841867228\n820 \t| Mean loss: 315.35457641165294\n830 \t| Mean loss: 315.3545781287676\n840 \t| Mean loss: 315.35457572040605\n850 \t| Mean loss: 315.3545743279764\n860 \t| Mean loss: 315.35457263598016\n870 \t| Mean loss: 315.35456582425974\n880 \t| Mean loss: 315.3545700231891\n890 \t| Mean loss: 315.3545564870025\nTime used for the training:  672.5232860000001 s\nTest Result:  tensor(12305.1288, grad_fn=<SumBackward0>)\n"
    }
   ],
   "source": [
    "def MSES(model_dm, target_dm):\n",
    "\n",
    "    loss = (model_dm - target_dm.view_as(model_dm)) ** 2\n",
    "    loss = loss.view(loss.size()[0], 1, -1)\n",
    "\n",
    "    return torch.sum(torch.sum(loss, dim=2) ** 2)\n",
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [16, 32, 64]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"AE_SGD_\" + str(i)+ \"_\" + str(neuron) + \"_distance\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = AutoEncoder([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        lossFun = MSES\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        for i in range(3):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            helper.train(dlr, EPOCH, print_on_each=10)\n",
    "            helper.backup()\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\")    \n",
    "            \n",
    "            print(\"Test Result: \", \n",
    "                            test_model(helper.model.encode, helper.preprocess, test_data, (N, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utils.plot_records(\n",
    "    helper.records[\n",
    "        ['loss_mean', 'loss_max', 'loss_min']\n",
    "    ].to_dict(orient='list'), \n",
    "    helper.epoch, value_label='loss')\n",
    "\n",
    "utils.plot_records(\n",
    "    helper.records[['train_time']].to_dict(orient='list'), \n",
    "    helper.epoch, value_label='train_time')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}