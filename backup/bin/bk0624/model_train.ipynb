{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "reload(sys.modules['utils']);\n",
    "reload(sys.modules['trainHelper']);\n",
    "reload(sys.modules['lossFunction']);\n",
    "reload(sys.modules['model.RNN']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import trainHelper\n",
    "import utils\n",
    "import numpy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from model.AutoEncoder import AutoEncoder\n",
    "from model.Linear import Linear\n",
    "\n",
    "import model.RNN as rnn\n",
    "\n",
    "import os.path \n",
    "import glob\n",
    "\n",
    "from lossFunction import CoordsToDMLoss, ReconLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = (1000, 1)\n",
    "ss, N, d = 1600, 10, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    euclidean_data1 = utils.load_variable('data/euclidean_data1.pkl')\n",
    "    euclidean_data2 = utils.load_variable('data/euclidean_data2.pkl')\n",
    "    \n",
    "    rand_data1 = utils.load_variable('data/rand_data1.pkl')\n",
    "    rand_data2 = utils.load_variable('data/rand_data2.pkl')\n",
    "    \n",
    "    if euclidean_data1.size() != (ss, 1, N, N):\n",
    "        raise Exception(\"Previous data not match requirement !\")\n",
    "\n",
    "except:\n",
    "    \n",
    "    print(\"Updated data for requirement !\")\n",
    "\n",
    "    euclidean_data1 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    euclidean_data2 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data1 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data2 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    utils.dump_variable(euclidean_data1, 'data/euclidean_data1.pkl')\n",
    "    utils.dump_variable(euclidean_data2, 'data/euclidean_data2.pkl')\n",
    "\n",
    "    utils.dump_variable(rand_data1, 'data/rand_data1.pkl')\n",
    "    utils.dump_variable(rand_data2, 'data/rand_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([\n",
    "            # euclidean_data1,\n",
    "            # euclidean_data2,\n",
    "            rand_data1,\n",
    "            rand_data2,\n",
    "        ]) \n",
    "\n",
    "data = data.view(ss * 2, 1, N, N)\n",
    "\n",
    "data = utils.minmax_norm(data, dmin=0)[0]\n",
    "data = torch.tensor(data.data, requires_grad=True)\n",
    "\n",
    "batch = 32\n",
    "dlr = DataLoader(data, batch_size=batch, shuffle=True)\n",
    "\n",
    "data[0, 0, :3, :3].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(helper):\n",
    "    for i in range(10):\n",
    "\n",
    "        EPOCH = 100\n",
    "            \n",
    "        print(\"Training \", helper.id)\n",
    "\n",
    "        helper.train(dlr, EPOCH, print_on_each=49)\n",
    "        helper.backup()\n",
    "            \n",
    "        min_lr = helper.records['lr'].min()\n",
    "            \n",
    "        print(\"Time used for the training: \", \n",
    "                    helper.records['train_time'].sum(), \"s\", \n",
    "                \"| Last lr: \", min_lr)\n",
    "\n",
    "        if 1e-7 > min_lr and helper.records['lr'][helper.records['lr'] == min_lr].count() >= EPOCH:\n",
    "            print(\"Reach Low learning Rate !\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    N = x.size()[-1]\n",
    "    x = x.view(-1, N, N)\n",
    "    return torch.tensor(x.data, requires_grad=True)\n",
    "\n",
    "for neuron in range(16, 97, 16):\n",
    "\n",
    "    for i in range(1, 4):\n",
    "\n",
    "        model_id = \"Coord_bRNN_\" + str(i)+ \"_\" + str(neuron) + \"_M_MSE\"\n",
    "\n",
    "        model = rnn.bRNN(N, 2, [N, *[neuron for j in range(i)], int(neuron / 2)], num_rnn_layers=1)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        lossFun = CoordsToDMLoss(N, 2, lossFun=nn.MSELoss(reduction='sum'))\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id, \n",
    "            model=model, optimizer=optimizer, preprocess=preprocess, lossFun=lossFun)\n",
    "\n",
    "        train(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in range(16, 97, 16):\n",
    "\n",
    "    for i in range(0, 6):\n",
    "\n",
    "        model_id = \"Coord_Linear_\" + str(i)+ \"_\" + str(neuron) + \"_D_MSE\"\n",
    "\n",
    "        in_dim = int((N * N - N) / 2)\n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = Linear([in_dim, *[neuron, int(neuron / 2) for j in range(i)], out_dim],\n",
    "                 activation=nn.LeakyReLU, final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "        \n",
    "        lossFun = CoordsToDMLoss(N, 2, lossFun=nn.MSELoss(reduction='sum'))\n",
    "        \n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, optimizer=optimizer, preprocess=preprocess, lossFun=lossFun)\n",
    "        \n",
    "        train(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(x.data, requires_grad=True)\n",
    "    \n",
    "for neuron in [32, 64, 96]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"Coord_Linear_\" + str(i)+ \"_\" + str(neuron) + \"_M_MSE\"\n",
    "\n",
    "        in_dim = N\n",
    "        out_dim = 2\n",
    "\n",
    "        model = Linear([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 activation=nn.LeakyReLU, \n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        lossFun = CoordsToDMLoss(N, 2)\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, optimizer=optimizer, preprocess=preprocess, lossFun=lossFun)\n",
    "        \n",
    "        train(helper)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    batch = x.size()[0]\n",
    "    N = x.size()[-1]\n",
    "    ex = utils.eignmatrix(x.view(batch, N, N)).view(batch, 1, N, N * 2)\n",
    "    return torch.tensor(ex.data, requires_grad=True)\n",
    "\n",
    "for neuron in [32]:\n",
    "\n",
    "    for i in range(6, 12):\n",
    "\n",
    "        model_id = \"Linear_\" + str(i)+ \"_\" + str(neuron) + \"_E_MSE\"\n",
    "\n",
    "        in_dim = N * 2\n",
    "        out_dim = 2\n",
    "\n",
    "        model = Linear([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 activation=nn.LeakyReLU, \n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "        \n",
    "        lossFun = CoordsToDMLoss(N, 2)\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, optimizer=optimizer, preprocess=preprocess, lossFun=lossFun)\n",
    "        \n",
    "       train(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([\n",
    "            euclidean_data1,\n",
    "            euclidean_data2,\n",
    "            # rand_data1,\n",
    "            # rand_data2,\n",
    "        ])\n",
    "        \n",
    "data = data.view(ss * 2, 1, N, N)\n",
    "\n",
    "data = utils.minmax_norm(data, dmin=0)[0]\n",
    "data = torch.tensor(data.data, requires_grad=True)\n",
    "\n",
    "batch = 32\n",
    "dlr = DataLoader(data, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [32, 64, 96]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"Linear_AE_\" + str(i)+ \"_\" + str(neuron) + \"_distance_LReLU\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = int((N * N - N) /2) \n",
    "\n",
    "        model = Linear([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 activation=nn.LeakyReLU, \n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "        \n",
    "        lossFun = ReconLoss()\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        \n",
    "        for i in range(5):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            try: \n",
    "                helper.train(dlr, EPOCH, print_on_each=100)\n",
    "                helper.backup()\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\", \"| Last lr: \", helper.records['lr'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [32, 64, 96]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"AE_\" + str(i)+ \"_\" + str(neuron) + \"_distance_LReLU\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = AutoEncoder([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 activation=nn.LeakyReLU, \n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "        \n",
    "        lossFun = ReconLoss()\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        \n",
    "        for i in range(5):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            try: \n",
    "                helper.train(dlr, EPOCH, print_on_each=100)\n",
    "                helper.backup()\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\", \"| Last lr: \", helper.records['lr'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "raise Exception(\"End\")\n",
    "\n",
    "def MSES(model_dm, target_dm):\n",
    "    \n",
    "    model_dm = utils.minmax_norm(model_dm)[0]\n",
    "    return nn.functional.binary_cross_entropy(model_dm, target_dm, reduction='sum')\n",
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [32, 64, 96]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"VAE_\" + str(i)+ \"_\" + str(neuron) + \"_distance_LReLU\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = VAE([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                    activation=nn.LeakyReLU,\n",
    "                    final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        lossFun = lossF.VAELoss(lossFun=MSES)\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        for i in range(3):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            helper.train(dlr, EPOCH, print_on_each=100)\n",
    "            helper.backup()\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.triu(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}