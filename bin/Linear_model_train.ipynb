{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainHelper\n",
    "import utils\n",
    "from mds.lmds import landmarkMDS\n",
    "from mds.cmds import classicalMDS\n",
    "from mds.fastmap import fastmap\n",
    "import numpy\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import lossFunction as lossF\n",
    "from model.AutoEncoder import AutoEncoder\n",
    "from model.DynParam import DynParam\n",
    "from model.Linear import Linear\n",
    "from model.VAE import VAE\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "reload(sys.modules['utils']);\n",
    "reload(sys.modules['trainHelper']);\n",
    "reload(sys.modules['lossFunction']);\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = (1000000, 100)\n",
    "ss, N, d = 1600, 10, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    euclidean_data1 = utils.load_variable('data/euclidean_data1.pkl')\n",
    "    euclidean_data2 = utils.load_variable('data/euclidean_data2.pkl')\n",
    "    \n",
    "    rand_data1 = utils.load_variable('data/rand_data.pkl')\n",
    "    rand_data2 = utils.load_variable('data/rand_data.pkl')\n",
    "    \n",
    "    if euclidean_data.size() != (ss, 1, N, N):\n",
    "        print(\"Updated data for requirement !\")\n",
    "        raise Exception(\"Previous data not match requirement !\")\n",
    "\n",
    "except:\n",
    "    euclidean_data1 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    euclidean_data2 = utils.generate_euclidean_DM(\n",
    "        N=N, d=d,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data1 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    rand_data2 = utils.generate_rand_DM(\n",
    "        N=N,\n",
    "        sample_size=ss,\n",
    "        sample_space=sample_space, isInt=True)\n",
    "\n",
    "    utils.dump_variable(euclidean_data1, 'data/euclidean_data1.pkl')\n",
    "    utils.dump_variable(euclidean_data2, 'data/euclidean_data2.pkl')\n",
    "\n",
    "    utils.dump_variable(rand_data1, 'data/rand_data1.pkl')\n",
    "    utils.dump_variable(rand_data2, 'data/rand_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([\n",
    "            # euclidean_data1,\n",
    "            # euclidean_data2,\n",
    "            rand_data1,\n",
    "            rand_data2,\n",
    "        ])\n",
    "        \n",
    "data = data.view(ss * 2, 1, N, N)\n",
    "\n",
    "data = utils.minmax_norm(data, dmin=0)[0]\n",
    "\n",
    "batch = 32\n",
    "dlr = DataLoader(data, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def MSES(model_dm, target_dm):\n",
    "\n",
    "    loss = (model_dm - target_dm.view_as(model_dm)) ** 2\n",
    "    loss = loss.view(loss.size()[0], 1, -1)\n",
    "\n",
    "    return torch.sum(torch.sum(loss, dim=2) ** 2)\n",
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [16, 32, 64]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"Linear_SGD_\" + str(i)+ \"_\" + str(neuron) + \"_distance\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = Linear([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "        \n",
    "        lossFun = lossF.CoordsToDMLoss(target_dim=(N, 2), \n",
    "            lossFun=lossF.DistanceLoss(\n",
    "                lossFun=MSES\n",
    "            )\n",
    "        )\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        for i in range(3):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            helper.train(dlr, EPOCH, print_on_each=10)\n",
    "            helper.backup()\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\")    \n",
    "            \n",
    "            print(\"Test Result: \", \n",
    "                            test_model(helper.model, helper.preprocess, test_data, (N, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSES(model_dm, target_dm):\n",
    "\n",
    "    loss = (model_dm - target_dm.view_as(model_dm)) ** 2\n",
    "    loss = loss.view(loss.size()[0], 1, -1)\n",
    "\n",
    "    return torch.sum(torch.sum(loss, dim=2) ** 2)\n",
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [16, 32, 64]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"AE_SGD_\" + str(i)+ \"_\" + str(neuron) + \"_distance\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = AutoEncoder([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "        \n",
    "        lossFun = lossF.DistanceLoss(\n",
    "                lossFun=MSES\n",
    "                )\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        for i in range(3):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            helper.train(dlr, EPOCH, print_on_each=10)\n",
    "            helper.backup()\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\")    \n",
    "            \n",
    "            print(\"Test Result: \", \n",
    "                            test_model(helper.model, helper.preprocess, test_data, (N, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSES(model_dm, target_dm):\n",
    "\n",
    "    loss = (model_dm - target_dm.view_as(model_dm)) ** 2\n",
    "    loss = loss.view(loss.size()[0], 1, -1)\n",
    "\n",
    "    return torch.sum(torch.sum(loss, dim=2) ** 2)\n",
    "\n",
    "def preprocess(x):\n",
    "    return torch.tensor(\n",
    "        utils.vectorize_distance_from_DM(x).data, requires_grad=True)\n",
    "\n",
    "for neuron in [16, 32, 64]:\n",
    "\n",
    "    for i in range(2, 6):\n",
    "\n",
    "        model_id = \"VAE_SGD_\" + str(i)+ \"_\" + str(neuron) + \"_distance\"\n",
    "\n",
    "        in_dim = int((N * N - N) /2) \n",
    "        out_dim = N * 2\n",
    "\n",
    "        model = AutoEncoder([in_dim, *[neuron for j in range(i)], out_dim],\n",
    "                 final_activation=None)\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "        \n",
    "        lossFun = lossF.DistanceLoss(\n",
    "                lossFun=MSES\n",
    "                )\n",
    "\n",
    "        helper = trainHelper.TrainHelper(id=model_id,\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            preprocess=preprocess,\n",
    "            lossFun=lossFun, lr_factor=0.1)\n",
    "        \n",
    "        for i in range(3):\n",
    "\n",
    "            EPOCH = 300\n",
    "            \n",
    "            print(\"Training \", helper.id)\n",
    "\n",
    "            helper.train(dlr, EPOCH, print_on_each=10)\n",
    "            helper.backup()\n",
    "\n",
    "            print(\"Time used for the training: \", \n",
    "                            helper.records['train_time'].sum(), \"s\")    \n",
    "            \n",
    "            print(\"Test Result: \", \n",
    "                            test_model(helper.model, helper.preprocess, test_data, (N, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utils.plot_records(\n",
    "    helper.records[\n",
    "        ['loss_mean', 'loss_max', 'loss_min']\n",
    "    ].to_dict(orient='list'), \n",
    "    helper.epoch, value_label='loss')\n",
    "\n",
    "utils.plot_records(\n",
    "    helper.records[['train_time']].to_dict(orient='list'), \n",
    "    helper.epoch, value_label='train_time')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}